{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11bf1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Plotting Packages\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, AutoMinorLocator\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374c4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_Environment():\n",
    "    def __init__(self, rewards = None):\n",
    "        \n",
    "        # Defining the number of states & actions\n",
    "        self.num_states  = 4\n",
    "        self.num_actions = 2\n",
    "        \n",
    "        # Defining the rewards action dynamic \n",
    "        self.rewards = np.array([[3, 2, 1, 0],[2,1,3,10]]) \n",
    "\n",
    "        # Definining the Initial State Value function\n",
    "        self.S = np.zeros(self.num_states)\n",
    "\n",
    "        # Definining the Initial Equi-probable Random policy\n",
    "        # The first index represents the state\n",
    "        # The sexond index represents the action in any particular state\n",
    "        self.pi = np.ones((4, 2)) * 0.5\n",
    "\n",
    "        # DEBUG CODE | IGNORE\n",
    "        # self.S = np.arange(16).reshape((4, 4))\n",
    "        # self.pi = np.zeros((4, 4, 4))\n",
    "        # self.pi[0][0] = [1, 0, 0, 0]\n",
    "        # self.pi[0][3] = [0, 0, 0, 1]\n",
    "        # self.pi[2][2] = [0, 1, 0, 0]\n",
    "        # self.pi[3][0] = [0, 0, 1, 0]\n",
    "\n",
    "    \n",
    "        # Defining the transition probabilities {p(s'|s, a)}\n",
    "        temp_0 = np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0.2, 0.8, 0, 0],\n",
    "            [0, 0.2, 0.8, 0],\n",
    "            [0, 0, 0.2, 0.8]\n",
    "        ])\n",
    "\n",
    "        temp_1 = np.array([\n",
    "            [0, 0.2, 0.3, 0.5],\n",
    "            [0.5, 0, 0.5, 0],\n",
    "            [0, 0.5, 0, 0.5],\n",
    "            [0, 0.5, 0.5, 0]\n",
    "        ])\n",
    "        self.tran_probs = np.ones((2, 4, 4))\n",
    "        self.tran_probs[0, :, :] = temp_0\n",
    "        self.tran_probs[1, :, :] = temp_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae30ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(dis_factor, theta, max_iters, rewards = None):\n",
    "    env = MDP_Environment(rewards)\n",
    "    policy_stable = False\n",
    "    iters = 1\n",
    "\n",
    "    while not policy_stable and iters <= max_iters:\n",
    "        print(f\"\\nPolicy Iteration  | Iteration = {iters}\")\n",
    "\n",
    "        # Policy Evaluation (or Prediction)\n",
    "        evaluate_policy(env, dis_factor, theta)\n",
    "        \n",
    "        print(np.round(env.S, 2))\n",
    "        # Policy Improvement\n",
    "        policy_stable = improve_policy(env, dis_factor)\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    # DEBUG CODE | IGNORE\n",
    "    print(\"Optimal State Value Function:\")\n",
    "    print(np.round(env.S, 2))\n",
    "    print(\"Optimal Policy:\")\n",
    "    print(env.pi)\n",
    "\n",
    "    # Plot the optimal value function and optimal policy, once the algo has converged\n",
    "    #env.plot_states()\n",
    "    #env.plot_policy()\n",
    "\n",
    "    \n",
    "#state value for a given policy\n",
    "def evaluate_policy(env, dis_factor, theta):\n",
    "    n = 0\n",
    "    temp_delta = copy.copy(theta)\n",
    "\n",
    "    s_value=np.zeros(env.num_states)\n",
    "\n",
    "    while temp_delta >= theta :\n",
    "        temp_delta = 0\n",
    "        prev_s_value=s_value\n",
    "        for s in range(env.num_states):\n",
    "            temp_stval=prev_s_value[s]\n",
    "            s_value[s]=0\n",
    "            \n",
    "            for a in range(env.num_actions):\n",
    "        \n",
    "                for s_prime in range(env.num_states):\n",
    "                    s_value[s] +=env.pi[s][a] * env.tran_probs[a, s, s_prime] * (\n",
    "                        env.rewards[a][s] + dis_factor * prev_s_value[s_prime]\n",
    "                    )\n",
    "\n",
    "\n",
    "            temp_delta = max(temp_delta, abs(temp_stval- s_value[s]))\n",
    "\n",
    "        n += 1\n",
    "    \n",
    "    env.S=s_value\n",
    "\n",
    "def improve_policy(env, dis_factor):\n",
    "    print(f\"Policy Improvement\")\n",
    "    policy_stable = True\n",
    "    \n",
    "    old_pi=copy.copy(env.pi)\n",
    "    for s in range(env.num_states):\n",
    "        \n",
    "        max_val = 1e-5\n",
    "        for a in range(env.num_actions):\n",
    "            s_value=0\n",
    "            for s_prime in range(env.num_states):\n",
    "                s_value +=env.pi[s][a] * env.tran_probs[a, s, s_prime] * (\n",
    "                    env.rewards[a][s] + dis_factor * env.S[s_prime]\n",
    "                )\n",
    "\n",
    "            if(s_value>max_val):\n",
    "                max_val=s_value\n",
    "                a_star=a\n",
    "        \n",
    "        #updating policy\n",
    "        for a in range(env.num_actions):\n",
    "            if a==a_star :\n",
    "                env.pi[s][a]=1\n",
    "                env.pi[s][1-a]=0\n",
    "        \n",
    "\n",
    "    if not np.array_equal(old_pi, env.pi):\n",
    "        policy_stable =  False\n",
    "\n",
    "    return policy_stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55da113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy Iteration  | Iteration = 1\n",
      "[5.45 4.69 5.45 7.95]\n",
      "Policy Improvement\n",
      "\n",
      "Policy Iteration  | Iteration = 2\n",
      "[ 3.    3.22 12.04 16.86]\n",
      "Policy Improvement\n",
      "Optimal State Value Function:\n",
      "[ 3.    3.22 12.04 16.86]\n",
      "Optimal Policy:\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "## Initializing the variables\n",
    "# Discount Factor\n",
    "dis_factor = 0.9\n",
    "# Threshold for stopping the policy evaluation (or prediction) algo\n",
    "theta = 1e-2\n",
    "\n",
    "# Maximum Iterations for the Policy Iteration Algorithms\n",
    "max_iters = 20\n",
    "\n",
    "# Using the original rewards distribution\n",
    "policy_iteration(dis_factor, theta, max_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a26525c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4e0766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4, 5], [1, 8, 9]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5d315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
